{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupied-senegal",
   "metadata": {},
   "source": [
    "# Optimizing a mesh using a Differentiable Renderer\n",
    "\n",
    "Differentiable rendering can be used to optimize the underlying 3D properties, like geometry and lighting, by backpropagating gradients from the loss in the image space. In this tutorial, we optimize geometry and texture of a single object based on a dataset of rendered ground truth views. This tutorial demonstrates functionality in `kaolin.render.mesh`, including the key `dibr_rasterization` function.  See detailed [API documentation](https://kaolin.readthedocs.io/en/latest/modules/kaolin.render.mesh.html). Note that this script is didactic and is not meant as a production end-to-end example; for more examples using DIB-R differentiable renderer, see [this repository](https://github.com/nv-tlabs/DIB-R-Single-Image-3D-Reconstruction).\n",
    "\n",
    "In addition, we demonstrate the use of [Kaolin's 3D checkpoints and training visualization](https://kaolin.readthedocs.io/en/latest/modules/kaolin.visualize.html) with the [Omniverse Kaolin App](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin/user_manual.html).\n",
    "\n",
    "Before starting the tutorial please make sure that to have [examples/samples/rendered_clock.zip](examples/samples/rendered_clock.zip) uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q matplotlib\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import kaolin as kal\n",
    "\n",
    "# path to the rendered image (using the data synthesizer)\n",
    "rendered_path = \"data/Measuring_Cup_Dataset/\"\n",
    "# path to the output logs (readable with the training visualizer in the omniverse app)\n",
    "logs_path = 'logs/'\n",
    "\n",
    "# We initialize the timelapse that will store USD for the visualization apps\n",
    "timelapse = kal.visualize.Timelapse(logs_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epoch = 50\n",
    "batch_size = 1\n",
    "laplacian_weight = 0.03\n",
    "image_weight = 0.1\n",
    "mask_weight = 1.\n",
    "texture_lr = 5e-2\n",
    "vertice_lr = 5e-4\n",
    "scheduler_step_size = 20\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "texture_res = 400\n",
    "\n",
    "# select camera angle for best visualization\n",
    "test_batch_ids = [2, 5, 10]\n",
    "test_batch_size = len(test_batch_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-anderson",
   "metadata": {},
   "source": [
    "# Generating Training Data\n",
    "\n",
    "To optimize a mesh, typical training data includes RGB images and segmentation mask. One way to generate this data is to use the Data Generator in the [Omniverse Kaolin App](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin/user_manual.html#data-generator). We provide sample output of the app in `examples/samples/`.\n",
    "\n",
    "## Parse synthetic data\n",
    "We first need to parse the synthetic data generated by the omniverse app.\n",
    "The omniverse app generate 1 file per type of data (which can be depth map, rgb image, segmentation map), and an additional metadata json file.\n",
    "\n",
    "The json file contains two main fields:\n",
    "- camera_properties: Contains all the data related to camera setting such as \"clipping_range\", \"horizontal_aperture\", \"focal_length\", \"tf_mat\"\n",
    "- asset_transforms: Those are transformations that are applied by the [Omniverse Kaolin App](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin/user_manual.html#data-generator), such as rotation / translation between objects or normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = len(glob.glob(os.path.join(rendered_path,'*_rgb.png')))\n",
    "train_data = []\n",
    "for i in range(num_views):\n",
    "    data = kal.io.render.import_synthetic_view(\n",
    "        rendered_path, i, rgb=True, semantic=True)\n",
    "    train_data.append(data)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                         shuffle=True, pin_memory=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-korea",
   "metadata": {},
   "source": [
    "# Loading the Sphere Template\n",
    "\n",
    "The optimization starts from deforming an input template mesh according to the input image. We will use a sphere template that provides better performance on objects without topological holes. We use \"/kaolin/examples/samples/sphere.obj\" for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = kal.io.obj.import_mesh('../samples/sphere.obj', with_materials=True)\n",
    "mesh = mesh.to_batched().cuda()\n",
    "mesh.vertices = mesh.vertices * 0.75  # adjust initial size\n",
    "mesh.vertices.requires_grad = True\n",
    "\n",
    "texture_map = torch.ones(\n",
    "    (1, 3, texture_res, texture_res), dtype=torch.float,\n",
    "    device='cuda', requires_grad=True)\n",
    "\n",
    "# The topology of the mesh and the uvs are constant\n",
    "# so we can initialize them on the first iteration only\n",
    "timelapse.add_mesh_batch(\n",
    "    iteration=0,\n",
    "    category='optimized_mesh',\n",
    "    faces_list=[mesh.faces.cpu()],\n",
    "    uvs_list=[mesh.uvs[0, ...].cpu()],\n",
    "    face_uvs_idx_list=[mesh.face_uvs_idx[0, ...].cpu()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-findings",
   "metadata": {},
   "source": [
    "# Preparing the losses and regularizer\n",
    "\n",
    "During training we will use different losses:\n",
    "- an image loss: an L1 loss based on RGB image.\n",
    "- a mask loss: an Intersection over Union (IoU) of the segmentation mask with the soft_mask output by DIB-R rasterizer.\n",
    "- a laplacian loss: to avoid deformation that are too strong.\n",
    "- a flat loss: to keep a smooth surface and avoid faces intersecting.\n",
    "\n",
    "For that we need to compute the laplacian matrix and some adjacency information\n",
    "(the face idx of faces connected to each edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate vertices center as a learnable parameter\n",
    "vertices_init = mesh.vertices.clone().detach()\n",
    "vertices_init.requires_grad = False\n",
    "\n",
    "# This is the center of the optimized mesh, separating it as a learnable parameter helps the optimization. \n",
    "vertice_shift = torch.zeros((3,), dtype=torch.float, device='cuda',\n",
    "                            requires_grad=True)\n",
    "\n",
    "nb_faces = mesh.faces.shape[0]\n",
    "nb_vertices = vertices_init.shape[1]\n",
    "\n",
    "## Set up auxiliary laplacian matrix for the laplacian loss\n",
    "vertices_laplacian_matrix = kal.ops.mesh.uniform_laplacian(\n",
    "    nb_vertices, mesh.faces) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-cross",
   "metadata": {},
   "source": [
    "# Setting up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_optim  = torch.optim.Adam(params=[mesh.vertices, vertice_shift],\n",
    "                                   lr=vertice_lr)\n",
    "texture_optim = torch.optim.Adam(params=[texture_map], lr=texture_lr)\n",
    "vertices_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    vertices_optim,\n",
    "    step_size=scheduler_step_size,\n",
    "    gamma=scheduler_gamma)\n",
    "texture_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    texture_optim,\n",
    "    step_size=scheduler_step_size,\n",
    "    gamma=scheduler_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda68e5",
   "metadata": {},
   "source": [
    "# Set up Differentiable Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(in_batch_size, cam_proj, cam_transform, img_shape):\n",
    "    ### Prepare mesh data with projection regarding to camera ###\n",
    "    vertices_batch = kal.ops.pointcloud.center_points(mesh.vertices) + vertice_shift\n",
    "\n",
    "    face_vertices_camera, face_vertices_image, face_normals = \\\n",
    "        kal.render.mesh.prepare_vertices(\n",
    "            vertices_batch.repeat(in_batch_size, 1, 1),\n",
    "            mesh.faces, cam_proj, camera_transform=cam_transform)\n",
    "\n",
    "    ### Perform Rasterization ###\n",
    "    # Construct attributes that DIB-R rasterizer will interpolate.\n",
    "    # the first is the UVS associated to each face\n",
    "    # the second will make a hard segmentation mask\n",
    "    face_attributes = [\n",
    "        mesh.face_uvs.repeat(in_batch_size, 1, 1, 1),\n",
    "        torch.ones((in_batch_size, nb_faces, 3, 1), device='cuda')\n",
    "    ]\n",
    "\n",
    "    # If you have nvdiffrast installed you can change rast_backend to\n",
    "    # nvdiffrast or nvdiffrast_fwd\n",
    "    image_features, soft_mask, face_idx = kal.render.mesh.dibr_rasterization(\n",
    "        img_shape[0], img_shape[1], face_vertices_camera[:, :, :, -1],\n",
    "        face_vertices_image, face_attributes, face_normals[:, :, -1],\n",
    "        rast_backend='cuda')\n",
    "\n",
    "    # image_features is a tuple in composed of the interpolated attributes of face_attributes\n",
    "    texture_coords, mask = image_features\n",
    "    image = kal.render.mesh.texture_mapping(texture_coords,\n",
    "                                            texture_map.repeat(in_batch_size, 1, 1, 1), \n",
    "                                            mode='bilinear')\n",
    "    image = torch.clamp(image * mask, 0., 1.)\n",
    "    return image, soft_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-suggestion",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This toy tutorial optimizes geometry and texture of the mesh directly to demonstrate losses, rasterization and 3D checkpoints available in Kaolin.\n",
    "\n",
    "These components can be combined with a neural architecture of your choice to learn tasks like image to 3D mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-companion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        vertices_optim.zero_grad()\n",
    "        texture_optim.zero_grad()\n",
    "        gt_image = data['rgb'].cuda()\n",
    "        gt_mask = data['semantic'].cuda()\n",
    "        cam_transform = data['metadata']['cam_transform'].cuda()\n",
    "        cam_proj = data['metadata']['cam_proj'].cuda()\n",
    "        \n",
    "        ### Render\n",
    "        image, soft_mask = render(batch_size, cam_proj, cam_transform, gt_image.shape[1:])\n",
    "        \n",
    "        ### Compute Losses ###\n",
    "        image_loss = torch.mean(torch.abs(image - gt_image))\n",
    "        mask_loss = kal.metrics.render.mask_iou(soft_mask,\n",
    "                                                gt_mask.squeeze(-1))\n",
    "        # laplacian loss\n",
    "        vertices_mov = mesh.vertices - vertices_init\n",
    "        vertices_mov_laplacian = torch.matmul(vertices_laplacian_matrix, vertices_mov)\n",
    "        laplacian_loss = torch.mean(vertices_mov_laplacian ** 2) * nb_vertices * 3\n",
    "\n",
    "        loss = (\n",
    "            image_loss * image_weight +\n",
    "            mask_loss * mask_weight +\n",
    "            laplacian_loss * laplacian_weight\n",
    "        )\n",
    "        ### Update the mesh ###\n",
    "        loss.backward()\n",
    "        vertices_optim.step()\n",
    "        texture_optim.step()\n",
    "\n",
    "    vertices_scheduler.step()\n",
    "    texture_scheduler.step()\n",
    "    print(f\"Epoch {epoch} - loss: {float(loss)}\")\n",
    "    \n",
    "    ### Write 3D Checkpoints ###\n",
    "    pbr_material = [\n",
    "        {'rgb': kal.io.materials.PBRMaterial(diffuse_texture=torch.clamp(texture_map[0], 0., 1.))}\n",
    "    ]\n",
    "\n",
    "    vertices_batch = kal.ops.pointcloud.center_points(mesh.vertices) + vertice_shift\n",
    "\n",
    "    # We are now adding a new state of the mesh to the timelapse\n",
    "    # we only modify the texture and the vertices position\n",
    "    timelapse.add_mesh_batch(\n",
    "        iteration=epoch,\n",
    "        category='optimized_mesh',\n",
    "        vertices_list=[vertices_batch[0]],\n",
    "        materials_list=pbr_material\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-intranet",
   "metadata": {},
   "source": [
    "# Visualize training\n",
    "\n",
    "You can now use [the Omniverse app](https://docs.omniverse.nvidia.com/app_kaolin/app_kaolin) to visualize the mesh optimization over training by using the training visualizer on \"./logs/\", where we stored the checkpoints.\n",
    "\n",
    "You can also show the rendered image generated by DIB-R and the learned texture map with your 2d images libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f289f48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # This is similar to a training iteration (without the loss part)\n",
    "    data_batch = [train_data[idx] for idx in test_batch_ids]\n",
    "    cam_transform = torch.stack([data['metadata']['cam_transform'] for data in data_batch], dim=0).cuda()\n",
    "    cam_proj = torch.stack([data['metadata']['cam_proj'] for data in data_batch], dim=0).cuda()\n",
    "\n",
    "    image, soft_mask = render(test_batch_size, cam_proj, cam_transform, [256, 256])\n",
    "    \n",
    "    ## Display the rendered images\n",
    "    f, axarr = plt.subplots(1, test_batch_size, figsize=(7, 22))\n",
    "    f.subplots_adjust(top=0.99, bottom=0.79, left=0., right=1.4)\n",
    "    f.suptitle('DIB-R rendering', fontsize=30)\n",
    "    for i in range(test_batch_size):\n",
    "        axarr[i].imshow(image[i].cpu().detach())\n",
    "        \n",
    "## Display the texture\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('2D Texture Map', fontsize=30)\n",
    "plt.imshow(torch.clamp(texture_map[0], 0., 1.).cpu().detach().permute(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
