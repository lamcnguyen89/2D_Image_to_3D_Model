{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOtWla6ClR/GRBfu+rZJRa2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa4e876ac6b2409e95e5aa7c8e3dd2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e1e04c85c754446aab5ee901311fef4",
              "IPY_MODEL_f50d45aefcc943e0b691662f8d7055e9",
              "IPY_MODEL_413859ef0062479bbed4e718b8b31847"
            ],
            "layout": "IPY_MODEL_49114e09974d4bde8a70a9252404ea40"
          }
        },
        "8e1e04c85c754446aab5ee901311fef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9975aac6537940c289e2536da9d0e909",
            "placeholder": "​",
            "style": "IPY_MODEL_ae9749e7cce54faab8c2bc9cf7dd833e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f50d45aefcc943e0b691662f8d7055e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e060f6e3693c4c4f9d23c00c9c93bab9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9677cc2094f0484298468bc98428941b",
            "value": 2
          }
        },
        "413859ef0062479bbed4e718b8b31847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed96f7b24d494f7aa564151854ee6e01",
            "placeholder": "​",
            "style": "IPY_MODEL_6e4db1ffbb9342a8a9371418a98260e3",
            "value": " 2/2 [00:04&lt;00:00,  2.33s/it]"
          }
        },
        "49114e09974d4bde8a70a9252404ea40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9975aac6537940c289e2536da9d0e909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9749e7cce54faab8c2bc9cf7dd833e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e060f6e3693c4c4f9d23c00c9c93bab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9677cc2094f0484298468bc98428941b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed96f7b24d494f7aa564151854ee6e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e4db1ffbb9342a8a9371418a98260e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamcnguyen89/2D_Image_to_3D_Model/blob/main/Assignment_04/Assignment_04_Mistral_and_Mixtral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 4: Mistral vs Mixtral Pipeline**\n",
        "\n",
        "\n",
        "\n",
        "*   Got to https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
        "*   Colab should have both mixtral and mistral 7b pipelines\n",
        "*   Compare both in terms of speed and accuracy\n",
        "*   Bonus: 100 points for applying SMoE to other models\n",
        "*   Due 03Sep2024\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "epk6kUuPcsSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistral-7b** \\\n",
        "Mistral-7B is one of the biggest and most advanced Large Language Models out there, trained on a massive dataset of text and code.\n",
        "\n",
        "Link: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/13\n",
        "\n",
        "\n",
        "**Mixtral 8x7b** \\\n",
        "Mixtral 8x7b is a high-quality sparse mixture of experts (SMoE) model with open weights created by Mistral AI. It outperforms Llama 2 70B on most benchmarks and batches or beats GPT3.5 on most standard benchmarks.\n",
        "\n",
        "Link: https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb\n"
      ],
      "metadata": {
        "id": "zyAXeeKeeUg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Mistral-7B Model and Test**"
      ],
      "metadata": {
        "id": "T7N6ZBZ1O0-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Mistral-7B Model\n",
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install langchain\n",
        "!pip install datasets\n",
        "!pip install --upgrade huggingface_hub\n",
        "\n",
        "# You need to go onto Google Colab and create a variable in Secrets. This variable will contain a token for the huggingface account that we will get our pretrained models from\n",
        "from google.colab import userdata\n",
        "secret = userdata.get('HF_TOKEN')\n",
        "!huggingface-cli login --token {secret}\n",
        "\n",
        "clear_output()\n",
        "\n"
      ],
      "metadata": {
        "id": "ziBzTp2LlEL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQzj8hQgt9y-",
        "outputId": "0aaf4590-4fcd-47eb-a8d6-63cbe8024367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep  3 02:07:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "\n"
      ],
      "metadata": {
        "id": "EFbxeDluATgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pretrained model and load into memory\n",
        "# Note I required a GPU with 40gb of memory for this project.\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                             torch_dtype=\"auto\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                          torch_dtype=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "aa4e876ac6b2409e95e5aa7c8e3dd2c8",
            "8e1e04c85c754446aab5ee901311fef4",
            "f50d45aefcc943e0b691662f8d7055e9",
            "413859ef0062479bbed4e718b8b31847",
            "49114e09974d4bde8a70a9252404ea40",
            "9975aac6537940c289e2536da9d0e909",
            "ae9749e7cce54faab8c2bc9cf7dd833e",
            "e060f6e3693c4c4f9d23c00c9c93bab9",
            "9677cc2094f0484298468bc98428941b",
            "ed96f7b24d494f7aa564151854ee6e01",
            "6e4db1ffbb9342a8a9371418a98260e3"
          ]
        },
        "id": "w3qIw7EdAcTM",
        "outputId": "a3e20766-fc2b-4e34-d3ca-a34034a907b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa4e876ac6b2409e95e5aa7c8e3dd2c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to Prompt Mistral AI Models: https://community.aws/content/2dFNOnLVQRhyrOrMsloofnW0ckZ/how-to-prompt-mistral-ai-models-and-why\n",
        "\n",
        "\n",
        "text = \"<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\"\n",
        "\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "device = 'cuda'\n",
        "model_inputs = encodeds.to(device)\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rI344zEDcWA",
        "outputId": "c44c5668-410b-4795-8a67-01c47a18a6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Oh, absolutely! Here's a classic homemade mayonnaise recipe that's sure to impress:\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "* 2 large egg yolks\n",
            "* 1 tablespoon dijon mustard\n",
            "* 2 tablespoons white wine vinegar\n",
            "* 1 clove garlic, minced\n",
            "* 1/2 teaspoon salt\n",
            "* 1/2 cup olive oil\n",
            "* 1/4 cup vegetable oil\n",
            "* Fresh herbs or spices, to taste (optional)\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a small bowl, whisk together the egg yolks, dijon mustard, white wine vinegar, minced garlic, and salt until well combined.\n",
            "2. Slowly stream in the olive oil and vegetable oil, whisking continuously to create a smooth, creamy sauce.\n",
            "3. Taste and adjust seasoning as desired.\n",
            "4. Store any leftover mayonnaise in an airtight container in the fridge for up to 3 days.\n",
            "\n",
            "And there you have it! A simple, yet delicious homemade mayonnaise that's perfect for sandwiches, salads, and more. Enjoy!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Process Dataset**\n"
      ],
      "metadata": {
        "id": "HFijHlS9Oi48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://github.com/mddunlap924/PyTorch-LLM/blob/main/notebooks/training.ipynb\n",
        "!ls\n",
        "validation_dataset = load_dataset(\"Assignment_04_Dataset\")\n",
        "print(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI1VJNl0OeUg",
        "outputId": "b82446ed-e12b-467b-f00f-5c8dcc7ea414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment_04_Dataset  sample_data\n",
            "DatasetDict({\n",
            "    validation: Dataset({\n",
            "        features: ['Question', 'Answer_01', 'Answer_02', 'Answer_03', 'Answer_04', 'Answer_05'],\n",
            "        num_rows: 1516\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_JymuNxxFj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Mixtral Pipeline**"
      ],
      "metadata": {
        "id": "e8BdkjGExhd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "device='cuda'\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", quantization_config=True, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
        "\n",
        "prompt = \"My favourite condiment is\"\n",
        "\n",
        "model_inputs = tokenizer.apply_chat_template([prompt], return_tensors=\"pt\").to(device)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "id": "epmCDiRRqYga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}